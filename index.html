<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kefan Jin</title>

    <meta name="author" content="Kefan Jin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kefan Jin
                </p>
                <p>
		<!-- I'm a principal research scientist at <a href="https://deepmind.google/">Google DeepMind</a> in San Francisco, where I lead a small team that mostly works on <a href="https://www.matthewtancik.com/nerf">NeRF</a>.
		At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>, and <a href="https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/">Shopping</a>.
		I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>.
		I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>. -->

    I am a postdoctoral researcher in the <a href="https://sais.sjtu.edu.cn/">School of Automation and Intelligent Sensing</a> at Shanghai Jiao Tong University</a>, in the <a href="https://irmv.sjtu.edu.cn/">IRMV</a> Lab since January 2024, under the supervision of Professor <a href="https://irmv.sjtu.edu.cn/wanghesheng">He-Sheng Wang</a>. 
    My primary research focuses on the end-to-end navigation and robust perception for autonomous systems.

    Prior to this, I received my PhD in 2023 from the <a href="https://naoce.sjtu.edu.cn/">School of Ocean and Civil Engineering</a> at <a href="https://www.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, under the supervision of Professor <a href="https://naoce.sjtu.edu.cn/teachers/yihong.html">Hong Yi</a>. 
    During my PhD, I mainly researched autonomous navigation, self-adaptation, and cooperative navigation.
    
    My research interests lie in end-to-end navigation, robust navigation, robust perception, and cooperative formation.

                </p>
                <p style="text-align:center">
                  <a href="mailto:jinkefan@sjtu.edu.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV-JKF.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://www.zhihu.com/people/bi-an-85-25">Blog</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=aDp5KFEAAAAJ&view_op=list_works&sortby=pubdate">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://linkedin.com/in/kefan-jin">LinkedIn</a> 
                  <!-- <a href="https://github.com/jonbarron/">Github</a> -->
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images2/KefanJin.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images2/KefanJin.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am interested in the robustness issues in perception and navigation. Most of my research focuses on building world models to reconstruct and predict state information during navigation, aiming to enhance robustness against perception anomalies and motion disturbances.                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



            <tr onmouseout="TMECH_stop()" onmouseover="TMECH_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one" style="height: 120px">
                  <div class="two" id='TMECH_image' style="height: 120px">
                    <img src='images2/TMECH1.2.png' style="border-style: none;width:160px; height:auto;">
                  </div>
                  <img src='images2/TMECH1.2.png' style="border-style: none;width:160px; height:auto;">
                </div>
                <script type="text/javascript">
                  function TMECH_start() {
                    document.getElementById('TMECH_image').style.opacity = "1";
                  }
                  function TMECH_stop() {
                    document.getElementById('TMECH_image').style.opacity = "0";
                  }
                  TMECH_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/10951120">
                  <span class="papertitle">Unmanned Surface Vehicle Navigation Under Disturbances: World Model Enhanced Reinforcement Learning</span>
                </a>
                <br>
                <strong>Kefan Jin</strong>, Zhe Liu, Jian Wang, Hesheng Wang
                <br>
                <em>IEEE/ASME Transactions on Mechatronics</em>, 2025
                <br>
                <details class="paper-abstract">
                <summary>abstract</summary>
                <p>Rapidly developing deep reinforcement learning (DRL) methods are suited to address the increasingly complex task requirements of unmanned surface vehicles (USV) that traditional methods cannot resolve. However, current DRL methods fail to make full use of the USV dynamics characteristics and do not consider unknown external disturbances. This article presents a state prediction reinforcement learning (SPRL) framework, to enhance the robustness against disturbances in obstacle avoidance navigation tasks for USVs. A novel latent robust world model (LRWM) with double latent features is elaborated based on probabilistic graphical model (PGM) to learn the transition of the USV navigation and predict future latent features against disturbances. A motion feature is learned to capture the impact of environmental disturbances on USV dynamics, while a prediction feature is extracted to predict future motion states. The proposed SPRL method can provide both the actor and the critic with essential prior information, thereby facilitating its application to actor–critic-based DRL methods. We performed field experiments in a lagoon environment, demonstrating the proposed method's deployability on practical USV platforms and its effectiveness in obstacle avoidance navigation under unknown disturbances.</p>
                </details>

                <!-- <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10951120">pdf</a>
                /
                <a href="https://arxiv.org/abs/2412.15211">arXiv</a> -->
                <p></p>
              </td>
            </tr>

    <tr onmouseout="JFR_stop()" onmouseover="JFR_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one" style="height: 120px">
          <div class="two" id='JFR_image' style="height: 120px">
            <img src='images2/JFR1.1.png' style="border-style: none;width:160px; height:auto;">
          </div>
          <img src='images2/JFR1.1.png' style="border-style: none;width:160px; height:auto;">
        </div>
        <script type="text/javascript">
          function JFR_start() {
            document.getElementById('JFR_image').style.opacity = "1";
          }
          function JFR_stop() {
            document.getElementById('JFR_image').style.opacity = "0";
          }
          JFR_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/rob.22554">
          <span class="papertitle">Predictive obstacle avoidance algorithm for under‐actuated unmanned surface vehicle under disturbances via reinforcement learning</span>
        </a>
        <br>
        <strong>Kefan Jin</strong>, Zhe Liu, Jian Wang
        <br>
        <em>Journal of Field Robotics</em>, 2025
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Due to the growing complexity of diverse maritime tasks, underactuated unmanned surface vehicle (USV) has become a research hotspot. The rapid development of deep reinforcement learning (DRL) technology has brought forth a novel approach for the USV autonomous control, rendering unnecessary the dynamical modeling of the target USV. To further improve the USV collision avoidance performance against maritime disturbances, this paper presents a predictive reinforcement learning method for USV obstacle avoidance control. A prediction module is designed to generate latent features that depict environmental states. After that, the prediction feature is provided for a DRL-based policy module to produce an action distribution for the underactuated unmanned surface vehicle. The proposed method in this paper can enable the USV avoid obstacle and reach the destination solely based on its local observational information, without relying on prior global information. Simulation and physical experiments have demonstrated that, compared to general DRL methods, the proposed method exhibits stronger robustness to environmental disturbances, enabling the USV to reach the destination while avoid the obstacle.</p>
        </details>
      </td>
    </tr>



    <tr onmouseout="OE2_stop()" onmouseover="OE2_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='OE2_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images2/OE2.mp4" type="video/mp4">
          </video></div>
          <img src='images2/OE2.png' width="160">
        </div>
        <!-- <div class="one">
          <div class="two" id='r2r_image'><video  width=100% muted autoplay loop>
          <source src="images2/OE2.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images2/OE2.png' width=100%>
        </div> -->
        <script type="text/javascript">
          function OE2_start() {
            document.getElementById('OE2_image').style.opacity = "1";
          }

          function OE2_stop() {
            document.getElementById('OE2_image').style.opacity = "0";
          }
          OE2_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/pii/S0029801823023429?ref=pdf_download&fr=RR-2&rr=9983de61085785c1">
          <span class="papertitle">DEMRL: Dynamic estimation meta reinforcement learning for path following on unseen unmanned surface vehicle</span>
        </a>
        <br>
        <strong>Kefan Jin</strong>, Rui Gao, Jian Wang, Hongdong Wang, Hong Yi, C.-J. Richard Shi c
        <br>
        <em>Ocean Engineering</em>, 2023
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Reinforcement learning has been widely used for unmanned surface vehicle (USV) control tasks. However, the requirement of numerous training samples limits its transferability to new USVs. In this article, we propose a dynamic estimation meta reinforcement learning (DEMRL) approach that enables few-shot learning for the path following control policy. We first present a dynamic estimation method to learn a latent dynamic context feature. The learned context contains the hidden information of USV dynamics with only a few estimation samples. We then propose a meta reinforcement learning based training framework to learn the generalizable path following control policy. After that, given the prior knowledge from dynamic context, the well-trained policy can easily adapt to the target USV during the rapid adaptation process. This proposed method represents the initial effort in tackling the few-shot learning challenge associated with training reinforcement learning based USV path-following policies. Extensive experiments demonstrate that the proposed method can achieve promising path following performance for unseen USV with very few training data and training volume.</p>
        </details>
      </td>
    </tr>

    <tr onmouseout="OE1_stop()" onmouseover="OE1_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='OE1_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/OE1.png' width="160">
        </div>
        <script type="text/javascript">
          function OE1_start() {
            document.getElementById('OE1_image').style.opacity = "1";
          }

          function OE1_stop() {
            document.getElementById('OE1_image').style.opacity = "0";
          }
          OE1_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/pii/S0029801822013658">
          <span class="papertitle">Soft formation control for unmanned surface vehicles under environmental disturbance using multi-task reinforcement learning</span>
        </a>
        <br>
        <strong>Kefan Jin</strong>, Jian Wang, Hongdong Wang, Xiaofeng Liang, Yongjin Guo, Mianjin Wang, Hong Yi
        <br>
        <em>Ocean Engineering</em>, 2022
        <br>
        <p></p>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>This paper proposes a distributed soft formation collision avoidance strategy to address the problem of formation obstacle avoidance in complex scenario under the environment interference where unmanned surface vehicles (USVs) are restricted in observation and possess no global reference frame for localisation. A multi-task training framework for formation control is also developed based on the motion characteristics of USVs and the leader–follower method. The soft actor–critic (SAC) reinforcement learning algorithm is adapted to construct agents. With elaborated auxiliary tasks, the soft formation algorithm demonstrates strong resilience to disturbances caused by unknown environmental loads or obstacle avoidance needs with only partial information about the environmental state, thus maintaining the formation shape. The proposed trajectory communication system and policy sharing mechanism allow USVs to approach the destination and avoid collisions whilst maintaining a noncompact formation that can be broken up temporarily to improve obstacle avoidance and can be restored quickly when the obstacle is avoided. Trained agents can be applied to different formations of varying sizes. Simulation results demonstrate the feasibility and effectiveness of the proposed method in different complex sea scenarios and its robustness to unknown environmental disturbances.</p>
        </details>
      </td>
    </tr>

        <tr onmouseout="ICRA2_stop()" onmouseover="ICRA2_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ICRA2_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images2/ROBIO2.mp4" type="video/mp4">
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/ROBIO2.png' width="160">
        </div>
        <script type="text/javascript">
          function ICRA2_start() {
            document.getElementById('ICRA2_image').style.opacity = "1";
          }

          function ICRA2_stop() {
            document.getElementById('ICRA2_image').style.opacity = "0";
          }
          ICRA2_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9812411">
          <span class="papertitle">Graph Neural Network Based Relation Learning for Abnormal Perception Information Detection in Self-Driving Scenarios</span>
        </a>
        <br>
        <strong>Kefan Jin</strong>, Hongye Wang, Changxing Liu, Yu Zhai and Ling Tang
        <br>
        <em>ICRA</em>, 2022
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Robustness and safety concerns of perception systems are of great importance for autonomous vehicle navigation applications. Recent researches demonstrate that the surrounding dynamic object detection results of current perception systems can be easily interfered or attacked to mislead the navigation performance of the victim vehicle. In this paper, we develop a GNN based relation learning network to detect the abnormal information in the vehicle perception results, by investigating the relations among the surrounding dynamic objects and also the overall scenario information. Our underlying logic is that the motion of each surrounding object is also affected by its neighbors as well as the whole traffic scenario information, so there should exist a certain amount of consistency among those agents. Learning their spatiotemporal relations provides critical information for detecting the abnormal perception information. Experimental results on the standard CARLA simulator demonstrate our effectiveness in various scenarios and scalability to unseen cases.</p>
        </details>
      </td>
    </tr>

    <tr onmouseout="ICRA1_stop()" onmouseover="ICRA1_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ICRA1_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/ICRA1.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/ICRA1.png' width="145">
        </div>
        <script type="text/javascript">
          function ICRA1_start() {
            document.getElementById('ICRA1_image').style.opacity = "1";
          }

          function ICRA1_stop() {
            document.getElementById('ICRA1_image').style.opacity = "0";
          }
          ICRA1_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161507">
          <span class="papertitle">Anomaly Detection For Robust Autonomous Navigation</span>
        </a>
        <br>
        <strong>Kefan Jin</strong>, Fan Mu, Xingyao Han, Guangming Wang and Zhe Liu
        <br>
        <em>ICRA</em>, 2023
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Human drivers are remarkably robust against various unexpected occurring variations and corruptions by understanding temporal changes and traffic scenes. In contrast, the neural network based autonomous navigation system can be easily affected by sensor data anomaly, like occlusion, sensor noise, challenging weather and illumination conditions. Such external disturbances are inevitable in practical driving applications. In this paper, we develop a semi-supervised anomaly detection module to detect the corrupted data while extracting the traffic scenario features. We further introduce an end-to-end robust autonomous navigation framework based on the idea that the consecutive frames of clean data depict a similar traffic scenario and the differences among the sequential data imply the dynamic state changes. By taking into consideration both spatial traffic scenario and temporal environmental variation, the model is able to achieve robust navigation against sensor data corruptions. We conduct experiments in CARLA platform and the evaluation results show the effectiveness of the proposed method.</p>
        </details>
      </td>
    </tr>





    <tr onmouseout="ROBIO2_stop()" onmouseover="ROBIO2_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ROBIO2_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images2/ICRA2.mp4" type="video/mp4">
          </video></div>
          <img src='images2/ICRA2.png' width="160">
        </div>
        <script type="text/javascript">
          function ROBIO2_start() {
            document.getElementById('ROBIO2_image').style.opacity = "1";
          }

          function ROBIO2_stop() {
            document.getElementById('ROBIO2_image').style.opacity = "0";
          }
          ROBIO2_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/10011819">
          <span class="papertitle">Learning Robust Vehicle Navigation Policy Under Interference and Partial Sensor Failures</span>
        </a>
        <br>
        <strong>Kefan Jin*</strong>, Zhe Liu*, Yu Zhai and Yanzi Miao
        <br>
        <em>ROBIO</em>, 2022
        <br>
        <span style="color: red;">Best Conference Paper Award</span>
        <br>

        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Autonomous driving has been a research hotspot for several years. However, most researchers focus on the ideal driving condition, where the sensor data is always accurate and clean. In reality, the urban environment is complex and the sensor data can easily be disturbed, which can lead to failures of existing driving approaches. In this paper, we present a multi-sensor fusion based navigation framework to achieve robust navigation under interference and partial sensor failures, which implicitly learns the reliability of each input information and introduces graph neural networks to reconstruct features for robust navigation policy learning. Experiments on the high-fidelity CARLA platform demonstrate the effectiveness of our approach, especially our strong robustness against various disturbances on both the input point cloud data and the visual images.</p>
        </details>
      </td>
    </tr>


    <tr onmouseout="Arxiv1_stop()" onmouseover="Arxiv1_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='Arxiv1_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images2/arxiv.mp4" type="video/mp4">
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/arxiv.png' width="160">
        </div>
        <script type="text/javascript">
          function Arxiv1_start() {
            document.getElementById('Arxiv1_image').style.opacity = "1";
          }

          function Arxiv1_stop() {
            document.getElementById('Arxiv1_image').style.opacity = "0";
          }
          Arxiv1_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2203.09952">
          <span class="papertitle">Conquering Ghosts: Hidden Markov Model based Heterogeneous Relation Learning for Robust Navigation</span>
        </a>
        <br>
        <strong>Kefan Jin</strong>, Xingyao Han
        <br>
        <em>Arxiv</em>
        <br>

        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Environmental disturbances, such as sensor data noises, various lighting conditions, challenging weathers and external adversarial perturbations, are inevitable in real self-driving applications. Existing researches and testings have shown that they can severely influence the vehicles perception ability and performance, one of the main issue is the false positive detection, i.e., the ghost object which is not real existed or occurs in the wrong position (such as a non-existent vehicle). Traditional navigation methods tend to avoid every detected objects for safety, however, avoiding a ghost object may lead the vehicle into a even more dangerous situation, such as a sudden break on the highway. Considering the various disturbance types, it is difficult to address this issue at the perceptual aspect. A potential solution is to detect the ghost through relation learning among the whole scenario and develop an integrated end-to-end navigation system. Our underlying logic is that the behavior of all vehicles in the scene is influenced by their neighbors, and normal vehicles behave in a logical way, while ghost vehicles do not. By learning the spatio-temporal relation among surrounding vehicles, an information reliability representation is learned for each detected vehicle and then a robot navigation network is developed. In contrast to existing works, we encourage the network to learn how to represent the reliability and how to aggregate all the information with uncertainties by itself, thus increasing the efficiency and generalizability. To the best of the authors knowledge, this paper provides the first work on using graph relation learning to achieve end-to-end robust navigation in the presence of ghost vehicles. Simulation results in the CARLA platform demonstrate the feasibility and effectiveness of the proposed method in various scenarios.</p>
        </details>

      </td>
    </tr>


    <tr onmouseout="TMECH2_stop()" onmouseover="TMECH2_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='TMECH2_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/TMECH2.png' width="160">
        </div>
        <script type="text/javascript">
          function TMECH2_start() {
            document.getElementById('TMECH2_image').style.opacity = "1";
          }

          function TMECH2_stop() {
            document.getElementById('TMECH2_image').style.opacity = "0";
          }
          TMECH2_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/10958193">
          <span class="papertitle">Hierarchical reinforcement learning with model guidance for mobile manipulation</span>
        </a>
        <br>
        Yifan Zhou, Yixuan Zhou, <strong>Kefan Jin</strong>, Hesheng Wang
        <br>
        <em>IEEE/ASME Transactions on Mechatronics</em>, 2025
        <br>
        <!-- <a href="https://szymanowiczs.github.io/bolt3d">project page</a> -->
        <!-- / -->
        <!-- <a href="https://szymanowiczs.github.io/bolt3d">arXiv</a>  -->
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>This study presents a method for mobile manipulators to transport heavy and large objects by utilizing environmental support, thereby extending the capability of mobile manipulators to manipulate objects that exceed their payload capacity. This is a challenging task, as the agent must achieve whole-body coordination while satisfying multiple constraints, including payload limits, base collision avoidance, and joint limits for torque, velocity, and position. We propose a hierarchical deep reinforcement learning with model guidance to address the whole-body contact-rich mobile manipulation task. The system includes two main components: 1) high-level trajectory correction of end-effector based on dynamic movement primitives in operational space and 2) low-level redundancy resolution guided by a model-based safety filter, which filters out unsafe action and generates safe action by quadratic programming method. The proposed hierarchical structure can ensure the convergence of end-effector trajectory while solving the nonstationary transition problem during parallel training. Experiments in simulation and real-world show the system's effectiveness in achieving faster convergence, greater stability, and higher performance in mobile manipulation tasks compared to conventional methods.</p>
        </details>


      </td>
    </tr>

    <tr onmouseout="Symmetry_stop()" onmouseover="Symmetry_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='Symmetry_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/Symmetry.png' width="160">
        </div>
        <script type="text/javascript">
          function Symmetry_start() {
            document.getElementById('Symmetry_image').style.opacity = "1";
          }

          function Symmetry_stop() {
            document.getElementById('Symmetry_image').style.opacity = "0";
          }
          Symmetry_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://www.mdpi.com/2073-8994/17/6/848">
          <span class="papertitle">Timed-Elastic-Band Based Variable Splitting for Autonomous Trajectory Planning</span>
        </a>
        <br>
        Hao Zhu, <strong>Kefan Jin</strong>, Rui Gao, Jialin Wang and Richard Shi 
        <br>
        <em>Symmetry</em>
        <br>
        <!-- <a href="https://szymanowiczs.github.io/bolt3d">project page</a> -->
        <!-- / -->
        <!-- <a href="https://szymanowiczs.github.io/bolt3d">arXiv</a>  -->
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Existing trajectory planning methods often face challenges in ensuring stable robot motion control, leading to significant positional errors during navigation. This study proposes Timed-Elastic-Band-Based Variable Splitting (TEB-VS), a novel framework that integrates variable splitting (VS)—a constrained optimization technique—with the classical Timed-Elastic-Band (TEB) algorithm. Unlike incremental modifications to TEB, TEB-VS introduces a systematic combination of VS and TEB to decompose non-convex global constraints into tractable subproblems while leveraging symmetry principles for balanced multi-objective control (e.g., velocity, acceleration, and obstacle avoidance). Experimental results demonstrate that TEB-VS achieves a 46.5%
 improvement in motion stability over traditional TEB in obstacle-free simulations and a 37%
 enhancement in dynamic obstacle scenarios. Real-world tests show a 26.7%
 reduction in angular velocity oscillations, with computational efficiency comparable to TEB. The framework’s effectiveness in harmonizing trajectory smoothness and dynamic adaptability is validated through extensive simulations and TurtleBot2 experiments.</p>
        </details>

      </td>
    </tr>

    <!-- <tr onmouseout="ICGNC_stop()" onmouseover="ICGNC_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ICGNC_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images2/OE2.mp4" type="video/mp4">
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/OE2.png' width="160">
        </div>
        <script type="text/javascript">
          function ICGNC_start() {
            document.getElementById('ICGNC_image').style.opacity = "1";
          }

          function ICGNC_stop() {
            document.getElementById('ICGNC_image').style.opacity = "0";
          }
          ICGNC_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
          <span class="papertitle">Multi-agent Cluster Target Coverage Control Based on Hierarchical Planning</span>
        <br>
        Jinxuan Shi, Zhe Liu, <strong>Kefan Jin</strong>
        <br>
        <em>ICGNC</em>, 2024
        <br>
        <p></p>
        <p>
		By training a latent diffusion model to directly output 3D Gaussians we enable fast (~6 seconds on a single GPU) feed-forward 3D scene generation.
        </p>
      </td>
    </tr> -->


    <tr onmouseout="JOES_stop()" onmouseover="JOES_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='JOES_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/JOES.png' width="160">
        </div>
        <script type="text/javascript">
          function JOES_start() {
            document.getElementById('JOES_image').style.opacity = "1";
          }

          function JOES_stop() {
            document.getElementById('JOES_image').style.opacity = "0";
          }
          JOES_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/pii/S2468013322000109">
          <span class="papertitle">Construction and experimental verification research of a magnetic detection system for submarine pipelines based on a two-part towed platform</span>
        </a>
        <br>
        Mianjin Wang, Shikun Pang, <strong>Kefan Jin</strong>, Xiaofeng Liang, Hongdong Wang, Hong Yi
        <br>
        <em>Journal of Ocean Engineering and Science</em>, 2023
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>With the acceleration of the investigation and development of marine resources, the detection and location of submarine pipelines have become a necessary part of modern marine engineering. Submarine pipelines are a typical weak magnetic anomaly target, and their magnetic anomaly detection can only be realized within a certain distance. At present, a towfish or an autonomous underwater vehicle (AUV) is mainly used as the platform to equip magnetometers close to the submarine pipelines for magnetic anomaly detection. However, the mother ship directly affects the towfish, thus causing control interference. The AUV cannot detect in real time, which affects the magnetic anomaly detection and creates problems regarding detection efficiency. Meanwhile, a two-part towed platform has convenient control, thus reducing the interference of the towed mother ship and real-time detection. If the platform can maintain constant altitude sailing through the controller, the data accuracy in the actual magnetic anomaly detection can be guaranteed. On the basis of a two-part towed platform, a magnetic detection system with constant altitude sailing ability for submarine pipelines was constructed in this study. In addition, experimental verification was conducted. The experimental verification research shows that the constant altitude sailing experiment of the two-part towed platform verifies that the platform has good constant altitude sailing ability in both a hydrostatic environment and the actual marine environment. Meanwhile, the offshore magnetic anomaly detection experiment of submarine pipelines verifies the stable measurement function of the magnetic field and the function of the system to detect magnetic anomaly of submarine pipelines.</p>
        </details>
      </td>
    </tr>



    <tr onmouseout="CDC_stop()" onmouseover="CDC_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='CDC_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.
          </video></div>
          <img src='images2/CDC.png' width="160">
        </div>
        <script type="text/javascript">
          function CDC_start() {
            document.getElementById('CDC_image').style.opacity = "1";
          }

          function CDC_stop() {
            document.getElementById('CDC_image').style.opacity = "0";
          }
          CDC_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <!-- <a href="https://arxiv.org/abs/2203.09952"> -->
          <span class="papertitle">A Multi-Agent Reinforcement Learning Approach Based on Local Shapley Value</span>
        <!-- </a> -->
        <br>
        Jinxuan Shi, Zhe Liu, <strong>Kefan Jin</strong>, Hesheng Wang
        <br>
        <em>CDC</em>, 2025(accepted)
        <br>

        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Multi-Agent Reinforcement Learning (MARL)
has been extensively applied in cooperative control tasks,
yet credit assignment remains a critical challenge. Existing
approaches predominantly employ global reward sharing mechanisms
or rely on monolithic neural network architectures,
which may lead to inaccurate credit attribution for individual
agents and consequently result in suboptimal learning efficiency.
In this paper, we propose the Local Shapley Policy Optimization
(LSPO) method to address the multi-agent credit assignment
problem. The Shapley values are introduced to reasonably
assess the contribution of each agent. Furthermore, we present
a method for calculating local Shapley values to address the
issue of dimensional explosion, and we theoretically demonstrate
that local Shapley values can effectively approximate
global Shapley values. Extensive experiments conducted on
several cooperative multi-agent benchmarks demonstrate that
our approach achieves superior task performance compared to
existing state-of-the-art methods. Empirical results reveal that
the proposed LSPO framework substantially enhances both
learning efficiency and coordination capabilities in complex
multi-agent environments.</p>
        </details>

      </td>
    </tr>

    <tr onmouseout="ROBIO1_stop()" onmouseover="ROBIO1_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ROBIO1_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/ROBIO1.png' width="160">
        </div>
        <script type="text/javascript">
          function ROBIO1_start() {
            document.getElementById('ROBIO1_image').style.opacity = "1";
          }

          function ROBIO1_stop() {
            document.getElementById('ROBIO1_image').style.opacity = "0";
          }
          ROBIO1_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9739418">
          <span class="papertitle">Attacking End-to-End Visual Navigation Model: How Weak Existing Learning-Based Approaches Can Be?</span>
        </a>
        <br>
        Hongye Wang, <strong>Kefan Jin</strong>, Hesheng Wang
        <br>
        <em>ROBIO</em>, 2021
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Learning-based self-driving techniques and vehicle navigation approaches are the most hot topics in recent years. However, existing approaches typically assume the attack-free sensor data, the safety issue under large disturbance or external attack have not been well-solved. In this article, we build a simple FGSM-based attack method designed by minimizing the maximum value of the extracted visual features in order to greatly decrease the performance of the popular learning model for vehicle navigation and even make it totally fail. The proposed min-max operation based feature space attack method solves the problems of branch activation uncertainties and the lack of labels. Furthermore, we also provide a general adversarial training framework which can be used to overcome the proposed feature space attack. Simulational experiments in CARLA platform demonstrate the effectiveness and practical applicability of our attacking approach and defence strategy.</p>
        </details>
      </td>
    </tr>

        <tr onmouseout="AA_stop()" onmouseover="AA_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='AA_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/AA.png' width="160">
        </div>
        <script type="text/javascript">
          function AA_start() {
            document.getElementById('AA_image').style.opacity = "1";
          }

          function AA_stop() {
            document.getElementById('AA_image').style.opacity = "0";
          }
          AA_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://www.emerald.com/ria/article/41/6/714/5920/Map-less-long-term-localization-in-complex">
          <span class="papertitle">Map-less long-term localization in complex industrial environments</span>
        </a>
        <br>
        Zhe Liu, Zhijian Qiao, Chuanzhe Suo, Yingtian Liu, <strong>Kefan Jin</strong>
        <br>
        <em>Assembly Automation</em>, 2021
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Purpose – This paper aims to study the localization problem for autonomous industrial vehicles in the complex industrial environments. Aiming for
practical applications, the pursuit is to build a map-less localization system which can be used in the presence of dynamic obstacles, short-term and
long-term environment changes.
Design/methodology/approach – The proposed system contains four main modules, including long-term place graph updating, global localization
and re-localization, location tracking and pose registration. The first two modules fully exploit the deep-learning based three-dimensional point
cloud learning techniques to achieve the map-less global localization task in large-scale environment. The location tracking module implements the
particle filter framework with a newly designed perception model to track the vehicle location during movements. Finally, the pose registration
module uses visual information to exclude the influence of dynamic obstacles and short-term changes and further introduces point cloud registration
network to estimate the accurate vehicle pose.
Findings – Comprehensive experiments in real industrial environments demonstrate the effectiveness, robustness and practical applicability of the
map-less localization approach.
Practical implications – This paper provides comprehensive experiments in real industrial environments.
Originality/value – The system can be used in the practical automated industrial vehicles for long-term localization tasks. The dynamic objects,
short-/long-term environment changes and hardware limitations of industrial vehicles are all considered in the system design. Thus, this work moves
a big step toward achieving real implementations of the autonomous localization in practical industrial scenarios.</p>
        </details>

      </td>
    </tr>

        <tr onmouseout="PAG_stop()" onmouseover="PAG_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='PAG_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/PAG.png' width="160">
        </div>
        <script type="text/javascript">
          function PAG_start() {
            document.getElementById('PAG_image').style.opacity = "1";
          }

          function PAG_stop() {
            document.getElementById('PAG_image').style.opacity = "0";
          }
          PAG_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://link.springer.com/article/10.1007/s00024-021-02868-y">
          <span class="papertitle">Magnetic Anomaly Characteristics Analysis of a Finite-Length Submarine Cable</span>
        </a>
        <br>
        Mianjin Wang, Xiaofeng Liang, Hong Yi, <strong>Kefan Jin</strong>, Hongdong Wang
        <br>
        <em>Pure and Applied Geophysics</em>, 2021
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Submarine cables are a typical weak magnetic target. Studying the magnetic anomaly principle of these cables and establishing their magnetic anomaly model are the bases of realizing magnetic detection, location, and recognition. The magnetic anomaly model of a 2D, infinite-length, horizontal cylinder is traditional and typical. However, when the cable is laid on a polygonal line under the constraints of geographical conditions or it is broken, the traditional model is no longer applicable. In this study, a magnetic anomaly analysis model of a finite-length submarine cable was proposed based on magnetic anomaly theory of finite-length horizontal cylindrical shells. The characteristics, applicability, and magnetic anomaly variation law of the model were analyzed. Exploration of the critical length ratios for the magnetic anomaly of a finite-length submarine cable that cannot be calculated by the traditional model improves the present magnetic anomaly theory of submarine cables.</p>
        </details>

      </td>
    </tr>

            <tr onmouseout="TASE_stop()" onmouseover="TASE_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='TASE_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/TASE.png' width="160">
        </div>
        <script type="text/javascript">
          function TASE_start() {
            document.getElementById('TASE_image').style.opacity = "1";
          }

          function TASE_stop() {
            document.getElementById('TASE_image').style.opacity = "0";
          }
          TASE_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/abstract/document/9555922">
          <span class="papertitle">Visuomotor reinforcement learning for multirobot cooperative navigation</span>
        </a>
        <br>
        Zhe Liu, Qiming Liu, Ling Tang, <strong>Kefan Jin</strong>, Hongye Wang, Ming Liu, Hesheng Wang
        <br>
        <em>IEEE Transactions on Automation Science and Engineering</em>, 2021
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>This article investigates the multirobot cooperative navigation problem based on raw visual observations. A fully end-to-end learning framework is presented, which leverages graph neural networks to learn local motion coordination and utilizes deep reinforcement learning to generate visuomotor policy that enables each robot to move to its goal without the need of environment map and global positioning information. Experimental results show that, with a few tens of robots, our approach achieves comparable performance with the state-of-the-art imitation learning-based approaches with bird-view state inputs. We also illustrate our generalizability to crowded and large environments and our scalability to ten times number of the training robots. In addition, we demonstrate that our model trained for multirobot case can also improve the success rate in the single-robot navigation task in unseen environments. Note to Practitioners—With the development of intelligent industrial and logistic systems, robotic transportation systems are widely implemented. However, existing multirobot path coordination and navigation approaches are basically under some unreasonable assumptions, which are very hard to be implemented in practical scenarios. This article aims to greatly promote the real application of learning-based multirobot cooperative navigation approach, in order to achieve the following. First, we introduce an end-to-end reinforcement learning framework instead of the commonly used imitation learning strategy, as the latter one needs exhaustive training data to cover all the scenarios and does not have the required generalizability. Second, we directly use the raw sensor data instead of the commonly used bird-eye-view semantic observations, as the latter one is generally not representative of practical application scenario from the robot perspective and cannot solve the occlusion issue. Third, we interpret our learned model to illustrate which parts of the input and shared observations contribute most to the robots’ final actions. The above interpretability ensures predictability (thus safety) of our visuomotor policy in practical applications. Our learned visuomotor policy has the ability to coordinate dozens of robots by only using raw visual observations in unknown environments without map nor global localization information, this is the first time in the literature. Our future work includes solving the sim-to-real issue and conducting physical experiments.</p>
        </details>
      </td>
    </tr>

            <tr onmouseout="ISOPE_stop()" onmouseover="ISOPE_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ISOPE_image'><video  width=100% height=100% muted autoplay loop>
          <!-- <source src="images2/OE2.mp4" type="video/mp4"> -->
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/ISOPE.png' width="160">
        </div>
        <script type="text/javascript">
          function ISOPE_start() {
            document.getElementById('ISOPE_image').style.opacity = "1";
          }

          function ISOPE_stop() {
            document.getElementById('ISOPE_image').style.opacity = "0";
          }
          ISOPE_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://onepetro.org/ISOPEIOPEC/proceedings/ISOPE19/ISOPE19/ISOPE-I-19-460/21482">
          <span class="papertitle">End-to-end trajectory tracking algorithm for unmanned surface vehicle using reinforcement learning</span>
        </a>
        <br>
        <strong>Kefan Jin</strong>, Hongdong Wang, Hong Yi
        <br>
        <em>ISOPE</em>, 2019
        <br>
        <details class="paper-abstract">
        <summary>abstract</summary>
        <p>Autonomous motion control of USVs, especially in complex marine conditions, is always a fundamental problem. Conventional methods consider the USV hydrodynamic model and the influences of environmental disturbance separately. However, due to the randomness of wind, wave and current, the accumulative error of each model can be large. To address this issue, this paper presents an end-to-end USV tracking control method via deep reinforcement learning, where a modern Reinforcement learning algorithm Actor-Critic is adopted. Given no prior knowledge of the dynamical system, the proposed method takes as input the information of environment (e.g., speed of wind and flow, etc.), ship and target trajectory, then produces the ship control signal (i.e., rudder angle and forward momentum) directly. We further propose a customized reward function to appraise the performance of ship agent. The presented simulation results demonstrate that this novel algorithm performs well in tracking tasks under complex marine conditions which is designed to change constantly.</p>
        </details>

      </td>
    </tr>

            <!-- <tr onmouseout="bolt3d_stop()" onmouseover="bolt3d_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bolt3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images2/OE2.mp4" type="video/mp4">
          Your browser does not support the video tag.

          </video></div>
          <img src='images2/OE2.png' width="160">
        </div>
        <script type="text/javascript">
          function bolt3d_start() {
            document.getElementById('bolt3d_image').style.opacity = "1";
          }

          function bolt3d_stop() {
            document.getElementById('bolt3d_image').style.opacity = "0";
          }
          bolt3d_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2203.09952">
          <span class="papertitle">Key technologies and intelligence evolution of maritime UV</span>
        </a>
        <br>
        <strong>Kefan Jin</strong>, Hongdong Wang, Hong YI, Jingyang Liu, Jian Wang
        <br>
        <em>Chinese Journal of Ship Research</em>, 2019
        <br>
        <p></p>
        <p>
		By training a latent diffusion model to directly output 3D Gaussians we enable fast (~6 seconds on a single GPU) feed-forward 3D scene generation.
        </p>
      </td>
    </tr> -->





            <!-- <tr onmouseout="pnf_stop()" onmouseover="pnf_start()">
            <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
            <div class="two" id='pnf_image'>
              <img src='images/pnf_before.jpg' width="160"></div>
            <img src='images/pnf_after.jpg' width="160">
            </div>
            <script type="text/javascript">
            function pnf_start() {
              document.getElementById('pnf_image').style.opacity = "1";
            }

            function pnf_stop() {
              document.getElementById('pnf_image').style.opacity = "0";
            }
            pnf_stop()
            </script>
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="TODO">
            <span class="papertitle">Polynomial Neural Fields for Subband Decomposition</span>
            </a> <br>
            <a href="https://www.guandaoyang.com/">Guandao Yang*</a>,
            <a href="https://sagiebenaim.github.io/">Sagie Benaim*</a>,
            <a href="https://varunjampani.github.io/">Varun Jampani</a>,
            <a href="https://www.kylegenova.com/">Kyle Genova</a>,
            <strong>Jonathan T. Barron</strong>,
            <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
            <a href="http://home.bharathh.info/">Bharath Hariharan</a>,
            <a href="https://sergebelongie.github.io/">Serge Belongie</a>
            <br>
            <em>NeurIPS</em>, 2022
            <p>
            Representing neural fields as a composition of manipulable and interpretable components lets you do things like reason about frequencies and scale.
            </p>
            </td>
            </tr> 


            <tr onmouseout="malle_stop()" onmouseover="malle_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='malle_image'>
                    <img src='images/MalleConv_after.jpg' width="160"></div>
                  <img src='images/MalleConv_before.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function malle_start() {
                    document.getElementById('malle_image').style.opacity = "1";
                  }

                  function malle_stop() {
                    document.getElementById('malle_image').style.opacity = "0";
                  }
                  malle_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://yifanjiang.net/MalleConv.html">
                  <span class="papertitle">Fast and High-Quality Image Denoising via Malleable Convolutions</span>
                </a>
                <br>
                <a href="https://yifanjiang.net/">Yifan Jiang</a>,
                <a href="https://bartwronski.com/">Bartlomiej Wronski</a>, 
                <a href="https://bmild.github.io/">Ben Mildenhall</a>, <br>
                <strong>Jonathan T. Barron</strong>,
                <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/">Zhangyang Wang</a>,
                <a href="https://tianfan.info/">Tianfan Xue</a>
                <br>
                <em>ECCV</em>, 2022
                <br>
                <a href="https://yifanjiang.net/MalleConv.html">project page</a>
                /
                <a href="https://arxiv.org/abs/2201.00392">arXiv</a>
                <p></p>
                <p>
                We denoise images efficiently by predicting spatially-varying kernels at low resolution and using a fast fused op to jointly upsample and apply these kernels at full resolution.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="nerfsuper_stop()" onmouseover="nerfsuper_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='nerfsuper_image'><video  width=100% height=100% muted autoplay loop>
                  <source src="images/nerf_supervision.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                  </video></div>
                  <img src='images/nerf_supervision.jpg' width="160">
                </div>
                <script type="text/javascript">
                  function nerfsuper_start() {
                    document.getElementById('nerfsuper_image').style.opacity = "1";
                  }

                  function nerfsuper_stop() {
                    document.getElementById('nerfsuper_image').style.opacity = "0";
                  }
                  nerfsuper_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="http://yenchenlin.me/nerf-supervision/">
                  <span class="papertitle">NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</span>
                </a>
                <br>
                <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
                <a href="http://www.peteflorence.com/">Pete Florence</a>, 
                <strong>Jonathan T. Barron</strong>,  <br>
                <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, 
                <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
                <br>
                <em>ICRA</em>, 2022  
                <br>
                <a href="http://yenchenlin.me/nerf-supervision/">project page</a> / 
                <a href="https://arxiv.org/abs/2203.01913">arXiv</a> / 
                <a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /
                <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / 
                <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				
                <p></p>
                <p>NeRF works better than RGB-D cameras or multi-view stereo when learning object descriptors.</p>
              </td>
            </tr>

            <tr onmouseout="cvpr12_stop()" onmouseover="cvpr12_start()">
              <td style="padding:16px;width:20%;vertical-align:middle">
                <div class="one" style="height: 120px">
                  <div class="two" id='cvpr12_image' style="height: 120px">
                    <img src='images/BarronCVPR2012_after.jpg' style="border-style: none">
                  </div>
                  <img src='images/BarronCVPR2012_before.jpg' style="border-style: none">
                </div>
                <script type="text/javascript">
                  function cvpr12_start() {
                    document.getElementById('cvpr12_image').style.opacity = "1";
                  }

                  function cvpr12_stop() {
                    document.getElementById('cvpr12_image').style.opacity = "0";
                  }
                  cvpr12_stop()
                </script>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/17RfINbE2dr2EjXp9MtGO0MHJLQmQVhvT/view?usp=sharing">
                  <span class="papertitle">Shape, Albedo, and Illumination from a Single Image of an Unknown Object</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>CVPR</em>, 2012
                <br>
                <a href="https://drive.google.com/file/d/1Im_bUI42AP9VPoNtsjLajvtLRiwv39k3/view?usp=sharing">supplement</a> /
                <a href="data/BarronMalikCVPR2012.bib">bibtex</a> /
                <a href="https://drive.google.com/file/d/1IAlSF4k3_CEL9dfbaMiNTFPBoEkLhsRl/view?usp=sharing">poster</a>
                <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/B3DO.jpg" alt="b3do" width="160" style="border-style: none">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1_S8EQyngbHQrB415o0XkQ4V9SMzdEgWT/view?usp=sharing">
                  <span class="papertitle">A Category-Level 3-D Object Dataset: Putting the Kinect to Work</span>
                </a>
                <br>
                <a href="http://www.eecs.berkeley.edu/%7Eallie/">Allison Janoch</a>,
                <a href="http://sergeykarayev.com/">Sergey Karayev</a>,
                <a href="http://www.eecs.berkeley.edu/%7Ejiayq/">Yangqing Jia</a>,
                <strong>Jonathan T. Barron</strong>,
                <a href="http://www.cs.berkeley.edu/%7Emfritz/">Mario Fritz</a>,
                <a href="http://www.icsi.berkeley.edu/%7Esaenko/">Kate Saenko</a>,
                <a href="http://www.eecs.berkeley.edu/%7Etrevor/">Trevor Darrell</a>
                <br>
                <em>ICCV 3DRR Workshop</em>, 2011
                <br>
                <a href="data/B3DO_ICCV_2011.bib">bibtex</a> /
                <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a>
                <p>We present a large RGB-D dataset of indoor scenes and investigate ways to improve object detection using depth information.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/safs.jpg" alt="safs_small" width="160" height="160" style="border-style: none">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1EZTOO5xezLYcyIFgAzs4KuZFLbTcwTDH/view?usp=sharing">
                  <span class="papertitle">High-Frequency Shape and Albedo from Shading using Natural Image Statistics</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>CVPR</em>, 2011
                <br>
                <a href="data/BarronMalikCVPR2011.bib">bibtex</a>
                <p>This paper is subsumed by <a href="#SIRFS">SIRFS</a>.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/fast_texture.jpg" alt="fast-texture" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1rc05NatkQVmUDlGCAYcHSrvAzTpU9knT/view?usp=sharing">
                  <span class="papertitle">Discovering Efficiency in Coarse-To-Fine Texture Classification</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                <br>
                <em>Technical Report</em>, 2010
                <br>
                <a href="data/BarronTR2010.bib">bibtex</a>
                <p>A model and feature representation that allows for sub-linear coarse-to-fine semantic segmentation.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/prl.jpg" alt="prl" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/13rVuJpcytRdLYCnKpq46g7B7IzSrPQ2P/view?usp=sharing">
                  <span class="papertitle">Parallelizing Reinforcement Learning</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://www.eecs.berkeley.edu/~dsg/">Dave Golland</a>, <a href="http://www.cs.berkeley.edu/~nickjhay/">Nicholas J. Hay</a>
                <br>
                <em>Technical Report</em>, 2009
                <br>
                <a href="data/BarronPRL2009.bib">bibtex</a>
                <p>Markov Decision Problems which lie in a low-dimensional latent space can be decomposed, allowing modified RL algorithms to run orders of magnitude faster in parallel.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/bd_promo.jpg" alt="blind-date" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1PQjzKgFcrAesMIDJr-WDlCwuGUxZJZwO/view?usp=sharing">
                  <span class="papertitle">Blind Date: Using Proper Motions to Determine the Ages of Historical Images</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 136, 2008
                <p>Using the relative motions of stars we can accurately estimate the date of origin of historical astronomical images.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                  <span class="papertitle">Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 135, 2008
                <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
                <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
              </td>
            </tr>

          </tbody></table> -->

          
					<!-- <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
<!--             
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr> -->


            <!-- <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr> -->
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
